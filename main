

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from customDataset import PartDataset
import matplotlib.pyplot as plt


class AutoEncoder(nn.Module):

    def __init__(self):
        super().__init__()
        # N, 1, 128, 128
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, 3, stride=2, padding=1), #N, 16, 64,64
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1), #N, 32, 32,32
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # N, 64, 16,16
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # N, 128, 8,8
            nn.ReLU(),
            nn.Conv2d(128, 256, 3, stride=2, padding=1),  # N, 256, 4,4
            nn.ReLU(),
            nn.Conv2d(256, 512, 4)  # N, 512, 1,1

        )

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4),  # N, 256, 4,4
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),  # N, 128, 8,8
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),  # N, 64, 16,16
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # N, 32, 32,32
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # N, 16, 64,64
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),  # N, 1, 128, 128
            nn.Sigmoid()
        )


    def forward(self, x):
        encoded = self.encoder(x)

        # print('AAAAAH')
        # for i in range(len(self.encoder._modules)):
        #     print(x.shape, self.encoder._modules[str(i)])
        #     x = self.encoder._modules[str(i)](x)
        #     print(x.shape)
        decoded = self.decoder(encoded)
        return decoded


# Hyperparameters
num_epochs = 120
batch_size = 32
#lr = 0.002
optimizer_cls = optim.Adam


# Load data
dataset = PartDataset(csv_file="C:/Users/William/Desktop/set 1/set1.csv", root_dir='C:/Users/William/Desktop/set 1/images')# transform=transforms.ToTensor()

trainset, testset = torch.utils.data.random_split(dataset, [1000, 300])
test_loader = torch.utils.data.DataLoader(dataset=testset, shuffle=True, batch_size=batch_size, num_workers=0)
train_loader =torch.utils.data.DataLoader(dataset=trainset, shuffle=True, batch_size=batch_size, num_workers=0)
# Instantiate model
model = AutoEncoder()
loss_fn = nn.MSELoss()
#optimizer = torch.optim.Adam(model.parameters(), lr=lr)
outputs = []

# Training loop
for epoch in range(num_epochs):
    print("Epoch %d" % epoch)
    lr = 0.002
   # if epoch > 4:
    #    print('eyy')
     #   lr = 0.002*(2**(4-epoch))
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    for image, (label) in train_loader:

        recon = model(image)
        loss = loss_fn(recon, image)

        optimizer.zero_grad()

        loss.backward()
        optimizer.step()

    print(loss)
    #print("Loss = %.3f" % loss.data[0])
    outputs.append((epoch, image, recon))

for k in range(0, num_epochs, 4):
    plt.figure(figsize=(9, 2))
    plt.gray()
    imgs = outputs[k][1].detach().numpy()
    recon = outputs[k][2].detach().numpy()
    for i, item in enumerate(imgs):
        if i >= 9: break
        plt.subplot(2, 9, i + 1)
        # item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear
        # item: 1, 28, 28
        plt.imshow(item[0])

    for i, item in enumerate(recon):
        if i >= 9: break
        plt.subplot(2, 9, 9 + i + 1)  # row_length + i + 1
        # item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear
        # item: 1, 28, 28
        plt.imshow(item[0])

plt.show()
torch.save(model.state_dict(), 'C:/Users/William/Desktop/set 1/set1.pt')
